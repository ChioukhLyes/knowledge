# js
JavaScript, JS, twerkScript, ECMAscript, call it whatever you want. Here are
some neat party tricks you can perform at (including, but not limited to)
birthdays, job interviews and cat cafes.

## Function identifying with ES6 symbols
Checking if functions / objects are equal in JS is typically done with the
`===` operator. This checks if both functions point to the same location in
memory, and then returns a Boolean. In cases where you want to check if a
function or object is of a certain type (e.g. generated by a factory), you're
going to have a hard time. 

What you would want to do is attach a flag to the object to mark it as being a
certain type. Luckily with ES6 you can attach unique, enumerable flags (which 
means they don't show up when iterating over the keys) by using ES6 symbols.

Here's an example:
```js
const sym = Symbol('my unique string')

function generate () {
  const obj = {foo: 'bar'}
  obj[sym] = true
  return obj
}

const nw = generate()
const ot = generate()

nw === ot           // => false
nw[sym]             // => true

// don't ever do this, as if both values
// are undefined, it will also return true
nw[sym] === ot[sym]
```

## Only execute function if it exists
Cute little trick to only execute functions if they exist. Removes the need for
noop functions.
```js
function myFunc (fn) {
  fn && fn()
}
```

## Common module signatures
In order to form plug-and-play systems with swappable components it is key that
module signatures remain the same between modules. In staticly typed languages
it is possible to statically define the signatures for the modules, but this
doesn't work for js. In order to scratch that itch, the level community wrote
`abstract-leveldown`: a set of tests that can be used by implementers to enforce
an interface. So far it seems to succeed in it's goal, spreading to other
projects. Known projects to use this pattern are:
- [abstract-leveldown](https://www.npmjs.com/package/abstract-leveldown)
- [abstract-blob-store](https://github.com/maxogden/abstract-blob-store)

## Sync-or-async
Mocha has a neat little pattern that turns a function either sync or async
based on if the function expects a callback or not. The trick to doing this is
in using `Function.length`. Here's an example implementation of the pattern
mocha uses:
```js
// fn, fn -> null
function detect (fn, cb) {
  if (fn.length) return fn(() => cb())
  fn()
  cb()
}
```
- `cb` is the callback that is called when done
- `fn` is the main function that we're calling
- `fn.length` checks if `fn` expects an argument, and then passes a callback if it does
- if `fn` expects no arguments, we just call fn and `cb`

- [mocha/lib/runnable.js](https://github.com/mochajs/mocha/blob/master/lib/runnable.js)

## Find file in root of project
```js
const root = path.dirname(require.main.filename)
const localPackage = require(path.resolve(root + '/package.json'))
```

## Transducers
> Transduction: the action or process of converting something and especially
> energy or a message into another form

Transducers appear to be all the hype, providing faster `map` functions than are
included in JS by default. But how do they work? Easy!
```js
const arr = [1, 2, 3, 4]

// this code loops over
// the array twice
arr.map(val => val + 1).map(val => val % 3)

// this code loops over
// the array once thanks to
// transdusuper powers!
arr.map(val => {
  val = val + 1
  val = val % 3
  return val
})
```
Transducer libraries provide a set of predefined set of composable functions,
but you just as easily create your own.

In terms of performance we move from `On^maps` to `On`. This means that for
`n<=1`, no speed is gained. So keep in mind that if you're using transducers
for large data sets they only reduce the amount of passes, but not the speed
of the operations.

## Prototypes
Prototypes are a tricky beast; they work differently than classes and can be
tough to reason about. Luckily there'es a simple heuristic to remember how they
work: they're simply a linked list. Multiple inheritance is just muliple
pointers in the parent field.

## delete value from array
```js
const arr = [2, 5, 9]
const i = arr.indexOf(9)
if (i > -1) arr.splice(i, 1)
```

## Create a new promise
```js
const prom = new Promise((resolve, reject) => {
  resolve()
})
```

## Testing CLI applications
Either through `exec` or `pipe`.

__exec__
```js
const exec = require('child_process').exec
const fs = require('fs')

const cmd = 'echo ./* | ' + Bulk.cmd+' -c "pwd"'
var dir = __dirname + '/node_modules/@scoped'

exec(cmd, {cwd: dir}, function(err, stdout, stderr) {
  var dirs = fs.readdirSync(dir).map(function(item) {
    return path.join(dir, item)
  })
  t.ifError(err)
  t.deepEqual(stdout.trim().split(/\s+/g), dirs)
  t.end()
})
```

__pipe__
```js
const bulk = Bulk('pwd')
const bl = require('bl')

bulk.stdout
  .pipe(bl(function(err, chunk) {
    var stdout = chunk.toString()
    t.ifError(err)
    var expected = fs.readdirSync(dir).map(function(item) {
      return path.join(dir, item)
    })
    t.deepEqual(stdout.trim().split(/\s+/), expected)
    t.end()
  }))

bulk.stdin.end(dirs.join(' '))
```

## Streams
Streams are a control flow abstraction native to node to read, transform and
write data. Streams are composable, efficient and _easy_! Yes, really. As a
stream consumer, you can consider streams to be the simpler counterpart to
Promises. This section is meant to give you a pragmatic introduction to all
aspects of streams. If you're looking for a guide that covers _all_ aspects,
check out the [stream handbook](https://github.com/substack/stream-handbook) by
substack.

- [basics](#basics)
- [read-transform-write](#read-transform-write)
- [modules](#modules)
- [internals](#internals)
- [creating streams](#creating-streams)
- [resources](#see-also)

### basics
There are 4 types of streams:
- __read__: data can be read from (e.g. `fs.createReadStream`)
- __write__: data can be written to (e.g. `fs.createWriteStream`)
- __duplex__: data can be read from and data can be written to (e.g. websockets)
- __transform__: data can be written to, transformed and then read from (e.g.
  `gzip`)

Streams originate from shell, where commands can be piped together using the
`|` (pipe) character. Take this copy function:
```sh
$ cat ./my-file > ./other-file
```

Now using node streams:
```js
const fs = require('fs')

fs.createReadStream('./my-file')
  .pipe(fs.createWriteStream('./other-file'))
```

The stream reads from a file (`./my-file`) using a read stream and writes to a
file (`./other-file`) using a write stream. The speed is throttled to the
slowest operation (probably harddisk writes), and data is only requested when
the harddisk is ready to write a new chunk making it super efficient.

#### memory
Using streams reduces the amount of memory required by a node program, making
the program faster and less prone to crash. Using the regular `fs.readFile` /
`fs.readFileSync` api's with `'utf8'` will devour resources and eventually
crash on large files.

### read-transform-write
In shell / unix the most common operation is to read data from a source,
transform it, and then write it back out. In shell:
```sh
# read file, find lines containing `foo`, echo to stdout
$ cat ./my-file | grep 'foo'
```

The same program using streams:
```js
const through = require('through2')
const fs = require('fs')

fs.createReadStream('./my-file')
  .pipe(through(grep(/foo/)))
  .pipe(process.stdout)

// grep `transform` stream
// regex -> (buf, str, fn) -> null
function grep (regex) {
  return (chunk, enc, cb) => {
    if (regex.test(chunk.toString())) this.push(chunk)
    cb()
  }
}
```
This program does the same as the bash function. The reason why the code looks
more verbose is because:
1. we're importing dependencies explicitely rather than relying on globals
2. we implemented the `grep` function from scratch

To make sure you understand what's going on, let's break down the code:
1. we import the `fs` and `through2` modules (more on stream modules later)
2. we create a read stream from `./my-file`
3. we connect the read stream to a transform stream that filters our data
4. we connect our filtered data to `stdout`

The `grep` function works as follows:
1. `grep` takes a regex and returns a function that can be consumed by
   `through2`
2. `through2` creates a transform stream from a function
3. the function in `through2` gets a chunk (buffer), encoding (string) and
   callback (function) passed in
4. if the stringified buffer (`.toString()`) passed the regex check, push it to
   transform's stream read stream (e.g. to the next `.pipe()` function)
5. call the callback once done so the next chunk can be processed

And that's it! You should now understand what a read stream is, how to create
transform streams, and how to pipe data back out to either `stdout` or a file.

### modules
By design Node only exposes bare essentials, leaving much of the staple
functionality to live in userland (npm). If you're working with streams, these
are probably the packages you want to be using.

_These descriptions were extracted from
[mississippi](https://github.com/maxogden/mississippi), a convenience wrapper
for common streaming functions._

#### [pump](https://github.com/mafintosh/pump)
##### `pump(stream1, stream2, stream3, ..., cb)`

Pipes streams together and destroys all of them if one of them closes. Calls
`cb` with `(error)` if there was an error in any of the streams.

When using standard `source.pipe(destination)` the source will _not_ be
destroyed if the destination emits close or error. You are also not able to
provide a callback to tell when then pipe has finished.

`pump` does these two things for you, ensuring you handle stream errors
100% of the time (unhandled errors are probably the most common bug in most
node streams code)

```js
// lets do a simple file copy
var fs = require('fs')

var read = fs.createReadStream('./original.zip')
var write = fs.createWriteStream('./copy.zip')

// use pump instead of read.pipe(write)
pump(read, write, function (err) {
  if (err) return console.error('Copy error!', err)
  console.log('Copied successfully')
})
```

#### [pumpify](https://github.com/mafintosh/pumpify)
##### `pipeline = pumpify(stream1, stream2, stream3, ...)`

Builds a pipeline from all the transform streams passed in as arguments by
piping them together and returning a single stream object that lets you write
to the first stream and read from the last stream

If any of the streams in the pipeline emits an error or gets destroyed, or you
destroy the stream it returns, all of the streams will be destroyed and
cleaned up for you

```js
// first create some transform streams (note: these two modules are fictional)
const imageResize = require('image-resizer-stream')({width: 400})
const pngOptimizer = require('png-optimizer-stream')({quality: 60})

// instead of doing a.pipe(b), use pipeline
const resizeAndOptimize = pumpify(imageResize, pngOptimizer)
// `resizeAndOptimize` is a transform stream. when you write to it, it writes
// to `imageResize`. when you read from it, it reads from `pngOptimizer`.
// it handles piping all the streams together for you

// use it like any other transform stream
const fs = require('fs')

const read = fs.createReadStream('./image.png')
const write = fs.createWriteStream('./resized-and-optimized.png')

pump(read, resizeAndOptimize, write, function (err) {
  if (err) return console.error('Image processing error!', err)
  console.log('Image processed successfully')
})
```

#### [duplexify](https://github.com/mafintosh/duplexify)
##### `duplex = duplexify([writable, readable, opts])`

Take two separate streams, a writable and a readable, and turn them into a
single duplex (readable and writable) stream

The returned stream will emit data from the readable, and when you write to it,
it writes to the readable

You can either choose to supply the writable and the readable at the time you
create the stream, or you can do it later using the `.setWritable` and
`.setReadable` methods, and data written to the stream in the meantime will be
buffered for you

```js
const stdio = duplexify(process.stdout, process.stdin)
```

#### [through2](https://github.com/rvagg/through2)
##### `transform = through2(fn(chunk, enc, cb))`
`through2` is a module that allows you to create a transform stream from a
function, rather than a full prototype chain. It can operate on either buffers
or objects if `opt.objectMode = true`.
```js
const through = require('through2')
const csv = require('csv-parser')
const fs = require('fs')

fs.createReadStream('data.csv')
  .pipe(csv())
  .pipe(through.obj(throughFn))
  .pipe(process.stdout)

function throughFn (chunk, enc, cb) {
  const data = {
    name: chunk.name,
    address: chunk.address
  }
  cb(null, data)
}
```

#### [concat-stream](https://github.com/maxogden/concat-stream)
##### `file = concatStream(fn(data))`
Sometimes you need to operate on a full file rather than individual chunks.
`concat-stream` concatenates the full content of a stream and then calls the
callback. It is not a transform stream, more of a stream sink.

```js
const concatStream = require('concat-stream')
const fs = require('fs')

fs.createReadStream('./cat.jpg')
  .pipe(concatStream((data) => {
      // data is `cat.jpg` as a buffer
    }))
```

### internals
[tbi]

### creating streams
[tbi]

### resources
- [stream handbook](https://github.com/substack/stream-handbook) - stream guide by substack
- [mississippi](https://github.com/maxogden/mississippi) - a collection of useful stream utility modules
- [iojs/api/streams](https://iojs.org/api/stream.htm) - iojs streams documentation

## See Also
- [ES6 compat table](https://kangax.github.io/compat-table/es6/) - caniuse for js
- [js pitfalls](http://nrn.io/view/javascript-common-pitfalls)
- [crosswalk project](https://crosswalk-project.org/) - native app runtime
